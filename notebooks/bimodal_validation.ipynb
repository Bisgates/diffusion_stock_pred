{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bimodal Distribution Validation for Stock Diffusion Model\n",
    "\n",
    "## Objective\n",
    "Test whether the diffusion model can capture **multimodal distributions** rather than collapsing to mean prediction.\n",
    "\n",
    "## Experiment Design\n",
    "1. Create synthetic data with **identical input conditions** but **conflicting targets**\n",
    "   - Same 120-second history pattern\n",
    "   - Target A: +5% return (bullish outcome)\n",
    "   - Target B: -5% return (bearish outcome)\n",
    "2. Train the model on this conflicting data\n",
    "3. At inference, sample 500 times from the same condition\n",
    "4. Analyze the output distribution\n",
    "\n",
    "## Success Criteria\n",
    "- **Pass**: Output shows bimodal distribution (peaks at both +5% and -5%)\n",
    "- **Fail**: Output shows unimodal distribution at 0% (mean collapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.models.model import StockDiffusionModel, ModelConfig\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Synthetic Bimodal Dataset\n",
    "\n",
    "We create a dataset where:\n",
    "- All samples have the **exact same** input condition (120-step sequence)\n",
    "- Half the samples have target = +0.05 (5% up)\n",
    "- Half the samples have target = -0.05 (5% down)\n",
    "\n",
    "This creates a perfectly bimodal target distribution that the model must learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Synthetic dataset with identical conditions but bimodal targets.\n",
    "    \n",
    "    All samples share the same input sequence, but targets are either\n",
    "    +target_value or -target_value with equal probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples: int = 1000,\n",
    "        seq_length: int = 120,\n",
    "        n_features: int = 4,\n",
    "        target_value: float = 0.05,  # 5% return\n",
    "        condition_pattern: str = \"sine\"  # Pattern for the condition\n",
    "    ):\n",
    "        self.n_samples = n_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.n_features = n_features\n",
    "        self.target_value = target_value\n",
    "        \n",
    "        # Create a fixed condition pattern (same for all samples)\n",
    "        self.condition = self._create_condition(condition_pattern)\n",
    "        \n",
    "        # Create bimodal targets: half +target, half -target\n",
    "        self.targets = np.zeros(n_samples, dtype=np.float32)\n",
    "        self.targets[:n_samples // 2] = target_value   # Bullish\n",
    "        self.targets[n_samples // 2:] = -target_value  # Bearish\n",
    "        \n",
    "        # Shuffle to mix bullish and bearish samples\n",
    "        np.random.shuffle(self.targets)\n",
    "        \n",
    "        print(f\"Created bimodal dataset:\")\n",
    "        print(f\"  Samples: {n_samples}\")\n",
    "        print(f\"  Condition shape: ({seq_length}, {n_features})\")\n",
    "        print(f\"  Targets: +{target_value:.2%} or {-target_value:.2%}\")\n",
    "        print(f\"  Bullish samples: {(self.targets > 0).sum()}\")\n",
    "        print(f\"  Bearish samples: {(self.targets < 0).sum()}\")\n",
    "    \n",
    "    def _create_condition(self, pattern: str) -> np.ndarray:\n",
    "        \"\"\"Create a deterministic condition pattern.\"\"\"\n",
    "        t = np.linspace(0, 4 * np.pi, self.seq_length)\n",
    "        \n",
    "        if pattern == \"sine\":\n",
    "            # Sine wave pattern (simulates oscillating price)\n",
    "            features = np.zeros((self.seq_length, self.n_features), dtype=np.float32)\n",
    "            features[:, 0] = 0.01 * np.sin(t)  # log_return\n",
    "            features[:, 1] = 0.005 + 0.002 * np.sin(2 * t)  # volatility\n",
    "            features[:, 2] = 0.001 * np.ones(self.seq_length)  # spread\n",
    "            features[:, 3] = np.sin(t / 2)  # volume_zscore\n",
    "        elif pattern == \"trend_up\":\n",
    "            # Upward trend\n",
    "            features = np.zeros((self.seq_length, self.n_features), dtype=np.float32)\n",
    "            features[:, 0] = 0.001 * np.ones(self.seq_length)  # small positive returns\n",
    "            features[:, 1] = 0.005 * np.ones(self.seq_length)\n",
    "            features[:, 2] = 0.001 * np.ones(self.seq_length)\n",
    "            features[:, 3] = np.linspace(-1, 1, self.seq_length)\n",
    "        elif pattern == \"flat\":\n",
    "            # Flat/consolidation\n",
    "            features = np.zeros((self.seq_length, self.n_features), dtype=np.float32)\n",
    "            features[:, 0] = 0.0001 * np.random.randn(self.seq_length)  # near-zero returns\n",
    "            features[:, 1] = 0.003 * np.ones(self.seq_length)  # low volatility\n",
    "            features[:, 2] = 0.0005 * np.ones(self.seq_length)\n",
    "            features[:, 3] = np.zeros(self.seq_length)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pattern: {pattern}\")\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        return {\n",
    "            'input': torch.from_numpy(self.condition.copy()),  # Same condition for all\n",
    "            'target': torch.tensor([self.targets[idx]], dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "    def get_condition_tensor(self) -> torch.Tensor:\n",
    "        \"\"\"Get the shared condition as a tensor for inference.\"\"\"\n",
    "        return torch.from_numpy(self.condition.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "N_SAMPLES = 2000  # Total training samples\n",
    "SEQ_LENGTH = 120\n",
    "N_FEATURES = 4\n",
    "TARGET_VALUE = 0.05  # 5% return magnitude\n",
    "\n",
    "dataset = BimodalDataset(\n",
    "    n_samples=N_SAMPLES,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    n_features=N_FEATURES,\n",
    "    target_value=TARGET_VALUE,\n",
    "    condition_pattern=\"sine\"\n",
    ")\n",
    "\n",
    "# Visualize the condition and target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Plot condition pattern\n",
    "ax = axes[0]\n",
    "condition = dataset.condition\n",
    "feature_names = ['log_return', 'volatility', 'spread', 'volume_zscore']\n",
    "for i in range(N_FEATURES):\n",
    "    ax.plot(condition[:, i], label=feature_names[i], alpha=0.8)\n",
    "ax.set_xlabel('Time (seconds)')\n",
    "ax.set_ylabel('Feature Value')\n",
    "ax.set_title('Shared Input Condition (Same for All Samples)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot target distribution\n",
    "ax = axes[1]\n",
    "ax.hist(dataset.targets * 100, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(x=TARGET_VALUE * 100, color='green', linestyle='--', linewidth=2, label=f'+{TARGET_VALUE:.0%}')\n",
    "ax.axvline(x=-TARGET_VALUE * 100, color='red', linestyle='--', linewidth=2, label=f'-{TARGET_VALUE:.0%}')\n",
    "ax.set_xlabel('Target Return (%)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Target Distribution (Ground Truth: Bimodal)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Mean: {dataset.targets.mean():.6f} (should be ~0)\")\n",
    "print(f\"  Std:  {dataset.targets.std():.6f}\")\n",
    "print(f\"  Min:  {dataset.targets.min():.6f}\")\n",
    "print(f\"  Max:  {dataset.targets.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and Train the Diffusion Model\n",
    "\n",
    "We use a smaller model configuration for faster training on this simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (smaller for quick experiment)\n",
    "model_config = ModelConfig(\n",
    "    encoder_type=\"transformer\",\n",
    "    input_features=N_FEATURES,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    encoder_dim=32,       # Smaller encoder\n",
    "    encoder_layers=2,\n",
    "    encoder_heads=4,\n",
    "    diffusion_steps=500,  # Full diffusion steps\n",
    "    noise_schedule=\"cosine\",\n",
    "    prediction_type=\"epsilon\",\n",
    "    denoiser_type=\"mlp\",\n",
    "    denoising_hidden_dim=64,  # Smaller denoiser\n",
    "    denoising_blocks=3,\n",
    "    time_embedding_dim=32,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "model = StockDiffusionModel(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model parameters: {model.get_num_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    dataset: Dataset,\n",
    "    epochs: int = 100,\n",
    "    batch_size: int = 64,\n",
    "    learning_rate: float = 1e-3,\n",
    "    device: torch.device = device\n",
    ") -> list:\n",
    "    \"\"\"Train the diffusion model on the bimodal dataset.\"\"\"\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=epochs, eta_min=learning_rate / 10\n",
    "    )\n",
    "    \n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    pbar = tqdm(range(epochs), desc=\"Training\")\n",
    "    for epoch in pbar:\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            x_seq = batch['input'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_seq, target)\n",
    "            loss = outputs['loss']\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.6f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 200  # Enough epochs to converge on this simple task\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Epochs: {EPOCHS}, Batch size: {BATCH_SIZE}, LR: {LEARNING_RATE}\")\n",
    "print()\n",
    "\n",
    "losses = train_model(\n",
    "    model, \n",
    "    dataset, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample from the Trained Model\n",
    "\n",
    "Now we sample 500 times from the model given the **same condition** and analyze the output distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_predictions(\n",
    "    model: nn.Module,\n",
    "    condition: torch.Tensor,\n",
    "    n_samples: int = 500,\n",
    "    use_ddim: bool = True,\n",
    "    ddim_steps: int = 50,\n",
    "    device: torch.device = device\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sample multiple predictions from the diffusion model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained diffusion model\n",
    "        condition: Input condition tensor (seq_length, n_features)\n",
    "        n_samples: Number of samples to generate\n",
    "        use_ddim: Use DDIM sampling (faster)\n",
    "        ddim_steps: Number of DDIM steps if use_ddim=True\n",
    "        \n",
    "    Returns:\n",
    "        Array of sampled predictions (n_samples,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Expand condition for batch sampling\n",
    "    condition = condition.unsqueeze(0).to(device)  # (1, seq_len, features)\n",
    "    condition = condition.expand(n_samples, -1, -1)  # (n_samples, seq_len, features)\n",
    "    \n",
    "    # Sample using the model's sample method\n",
    "    samples = model.sample(\n",
    "        condition,\n",
    "        num_samples=1,  # Already batched\n",
    "        use_ddim=use_ddim,\n",
    "        ddim_steps=ddim_steps\n",
    "    )\n",
    "    \n",
    "    return samples.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the trained model\n",
    "N_INFERENCE_SAMPLES = 500\n",
    "\n",
    "print(f\"Sampling {N_INFERENCE_SAMPLES} predictions from the model...\")\n",
    "\n",
    "# Get the shared condition\n",
    "condition = dataset.get_condition_tensor()\n",
    "\n",
    "# Sample predictions\n",
    "predictions = sample_predictions(\n",
    "    model,\n",
    "    condition,\n",
    "    n_samples=N_INFERENCE_SAMPLES,\n",
    "    use_ddim=True,\n",
    "    ddim_steps=50\n",
    ")\n",
    "\n",
    "print(f\"\\nSampled {len(predictions)} predictions\")\n",
    "print(f\"Prediction statistics:\")\n",
    "print(f\"  Mean: {predictions.mean():.6f}\")\n",
    "print(f\"  Std:  {predictions.std():.6f}\")\n",
    "print(f\"  Min:  {predictions.min():.6f}\")\n",
    "print(f\"  Max:  {predictions.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Results: Is the Distribution Bimodal?\n",
    "\n",
    "We now compare the model's output distribution to the ground truth bimodal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bimodality(predictions: np.ndarray, target_value: float, threshold: float = 0.02):\n",
    "    \"\"\"\n",
    "    Analyze if the predictions show bimodal behavior.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Array of model predictions\n",
    "        target_value: Expected peak locations (+/- target_value)\n",
    "        threshold: Threshold for classifying predictions as bullish/bearish\n",
    "        \n",
    "    Returns:\n",
    "        dict with analysis results\n",
    "    \"\"\"\n",
    "    # Classify predictions\n",
    "    bullish = predictions > threshold\n",
    "    bearish = predictions < -threshold\n",
    "    neutral = ~bullish & ~bearish\n",
    "    \n",
    "    n_total = len(predictions)\n",
    "    n_bullish = bullish.sum()\n",
    "    n_bearish = bearish.sum()\n",
    "    n_neutral = neutral.sum()\n",
    "    \n",
    "    # Calculate bimodality score\n",
    "    # A perfect bimodal distribution would have ~50% bullish, ~50% bearish, 0% neutral\n",
    "    bimodal_score = min(n_bullish, n_bearish) / max(n_bullish, n_bearish, 1)\n",
    "    spread_score = 1 - (n_neutral / n_total)  # Higher is better (less neutral)\n",
    "    \n",
    "    # Mean absolute deviation from target peaks\n",
    "    bullish_deviation = np.abs(predictions[bullish] - target_value).mean() if n_bullish > 0 else float('inf')\n",
    "    bearish_deviation = np.abs(predictions[bearish] - (-target_value)).mean() if n_bearish > 0 else float('inf')\n",
    "    \n",
    "    results = {\n",
    "        'n_bullish': n_bullish,\n",
    "        'n_bearish': n_bearish,\n",
    "        'n_neutral': n_neutral,\n",
    "        'pct_bullish': n_bullish / n_total * 100,\n",
    "        'pct_bearish': n_bearish / n_total * 100,\n",
    "        'pct_neutral': n_neutral / n_total * 100,\n",
    "        'bimodal_score': bimodal_score,\n",
    "        'spread_score': spread_score,\n",
    "        'bullish_deviation': bullish_deviation,\n",
    "        'bearish_deviation': bearish_deviation,\n",
    "        'mean': predictions.mean(),\n",
    "        'std': predictions.std()\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze bimodality\n",
    "results = analyze_bimodality(predictions, TARGET_VALUE, threshold=TARGET_VALUE * 0.3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BIMODALITY ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(f\"  Bullish (> {TARGET_VALUE * 0.3:.1%}):  {results['n_bullish']:4d} ({results['pct_bullish']:.1f}%)\")\n",
    "print(f\"  Bearish (< {-TARGET_VALUE * 0.3:.1%}): {results['n_bearish']:4d} ({results['pct_bearish']:.1f}%)\")\n",
    "print(f\"  Neutral (middle):       {results['n_neutral']:4d} ({results['pct_neutral']:.1f}%)\")\n",
    "print(f\"\\nBimodality metrics:\")\n",
    "print(f\"  Balance score: {results['bimodal_score']:.3f} (1.0 = perfectly balanced)\")\n",
    "print(f\"  Spread score:  {results['spread_score']:.3f} (1.0 = no neutral predictions)\")\n",
    "print(f\"\\nPeak accuracy:\")\n",
    "print(f\"  Bullish peak deviation: {results['bullish_deviation']:.6f}\")\n",
    "print(f\"  Bearish peak deviation: {results['bearish_deviation']:.6f}\")\n",
    "print(f\"\\nOverall statistics:\")\n",
    "print(f\"  Mean: {results['mean']:.6f} (should be ~0 for balanced bimodal)\")\n",
    "print(f\"  Std:  {results['std']:.6f} (should be ~{TARGET_VALUE:.4f} for bimodal)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Histogram comparison\n",
    "ax = axes[0]\n",
    "ax.hist(dataset.targets * 100, bins=50, alpha=0.5, label='Ground Truth', color='blue', density=True)\n",
    "ax.hist(predictions * 100, bins=50, alpha=0.5, label='Model Predictions', color='orange', density=True)\n",
    "ax.axvline(x=TARGET_VALUE * 100, color='green', linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax.axvline(x=-TARGET_VALUE * 100, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax.axvline(x=0, color='gray', linestyle=':', linewidth=2, alpha=0.7, label='Mean (MSE collapse)')\n",
    "ax.set_xlabel('Return (%)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distribution Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Model predictions only (detailed)\n",
    "ax = axes[1]\n",
    "ax.hist(predictions * 100, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "ax.axvline(x=TARGET_VALUE * 100, color='green', linestyle='--', linewidth=2, label=f'Expected +{TARGET_VALUE:.0%}')\n",
    "ax.axvline(x=-TARGET_VALUE * 100, color='red', linestyle='--', linewidth=2, label=f'Expected -{TARGET_VALUE:.0%}')\n",
    "ax.axvline(x=0, color='gray', linestyle=':', linewidth=2, label='Mean = 0')\n",
    "ax.set_xlabel('Predicted Return (%)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'Model Output Distribution (n={len(predictions)})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scatter plot of predictions\n",
    "ax = axes[2]\n",
    "ax.scatter(range(len(predictions)), predictions * 100, alpha=0.5, s=10)\n",
    "ax.axhline(y=TARGET_VALUE * 100, color='green', linestyle='--', linewidth=2, label=f'+{TARGET_VALUE:.0%}')\n",
    "ax.axhline(y=-TARGET_VALUE * 100, color='red', linestyle='--', linewidth=2, label=f'-{TARGET_VALUE:.0%}')\n",
    "ax.axhline(y=0, color='gray', linestyle=':', linewidth=2)\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Predicted Return (%)')\n",
    "ax.set_title('Individual Predictions')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verdict\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL VERDICT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criteria for success:\n",
    "# 1. Both bullish and bearish predictions exist (>20% each)\n",
    "# 2. Neutral predictions are minority (<30%)\n",
    "# 3. Std is close to target_value (not collapsed to 0)\n",
    "\n",
    "is_bimodal = (\n",
    "    results['pct_bullish'] > 20 and \n",
    "    results['pct_bearish'] > 20 and\n",
    "    results['pct_neutral'] < 30 and\n",
    "    results['std'] > TARGET_VALUE * 0.5\n",
    ")\n",
    "\n",
    "is_collapsed = (\n",
    "    results['pct_neutral'] > 70 or\n",
    "    results['std'] < TARGET_VALUE * 0.3\n",
    ")\n",
    "\n",
    "if is_bimodal:\n",
    "    print(\"\\n[PASS] The model has learned a BIMODAL distribution!\")\n",
    "    print(\"\\nThe diffusion model correctly captures that the same input\")\n",
    "    print(\"condition can lead to different outcomes (bullish OR bearish).\")\n",
    "    print(\"This demonstrates the model's ability to learn probabilistic\")\n",
    "    print(\"distributions rather than just point estimates.\")\n",
    "elif is_collapsed:\n",
    "    print(\"\\n[FAIL] The model has COLLAPSED to mean prediction!\")\n",
    "    print(\"\\nThe model outputs are clustered around the mean (0),\")\n",
    "    print(\"indicating it has degenerated into an MSE-like regression.\")\n",
    "    print(\"This means the diffusion process is not working correctly.\")\n",
    "    print(\"\\nPossible causes:\")\n",
    "    print(\"  - Insufficient training\")\n",
    "    print(\"  - Model architecture issues\")\n",
    "    print(\"  - Noise schedule problems\")\n",
    "else:\n",
    "    print(\"\\n[PARTIAL] Results are inconclusive.\")\n",
    "    print(\"\\nThe model shows some bimodal behavior but not clearly.\")\n",
    "    print(\"Consider training longer or adjusting hyperparameters.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Additional Experiments\n",
    "\n",
    "Let's run a few more experiments to validate the model's behavior under different conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Compare DDPM vs DDIM sampling\n",
    "print(\"Comparing DDPM vs DDIM sampling...\")\n",
    "\n",
    "# DDIM sampling (fast)\n",
    "predictions_ddim = sample_predictions(\n",
    "    model, condition, n_samples=300, use_ddim=True, ddim_steps=50\n",
    ")\n",
    "\n",
    "# DDPM sampling (slow but more accurate)\n",
    "predictions_ddpm = sample_predictions(\n",
    "    model, condition, n_samples=300, use_ddim=False\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(predictions_ddim * 100, bins=30, alpha=0.7, label='DDIM (50 steps)')\n",
    "ax.axvline(x=TARGET_VALUE * 100, color='green', linestyle='--', linewidth=2)\n",
    "ax.axvline(x=-TARGET_VALUE * 100, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_title(f'DDIM Sampling (std={predictions_ddim.std():.4f})')\n",
    "ax.set_xlabel('Return (%)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.hist(predictions_ddpm * 100, bins=30, alpha=0.7, label='DDPM (500 steps)', color='orange')\n",
    "ax.axvline(x=TARGET_VALUE * 100, color='green', linestyle='--', linewidth=2)\n",
    "ax.axvline(x=-TARGET_VALUE * 100, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_title(f'DDPM Sampling (std={predictions_ddpm.std():.4f})')\n",
    "ax.set_xlabel('Return (%)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Different target separations\n",
    "print(\"\\nTesting model on different target separations...\")\n",
    "print(\"(Using same trained model with different synthetic targets)\\n\")\n",
    "\n",
    "# Test how well the model generalizes\n",
    "# Note: The model was trained on ±5%, let's see if it can handle ±3% or ±7%\n",
    "\n",
    "# Create datasets with different separations (for comparison only)\n",
    "separations = [0.03, 0.05, 0.07]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, sep in zip(axes, separations):\n",
    "    test_dataset = BimodalDataset(\n",
    "        n_samples=100,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        n_features=N_FEATURES,\n",
    "        target_value=sep,\n",
    "        condition_pattern=\"sine\"\n",
    "    )\n",
    "    \n",
    "    ax.hist(test_dataset.targets * 100, bins=30, alpha=0.7, label='Ground Truth')\n",
    "    ax.axvline(x=sep * 100, color='green', linestyle='--', linewidth=2)\n",
    "    ax.axvline(x=-sep * 100, color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_title(f'Target: ±{sep:.0%}')\n",
    "    ax.set_xlabel('Return (%)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: The model was trained on ±5% targets.\")\n",
    "print(\"For different separations, the model would need to be retrained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "### What This Experiment Tests\n",
    "\n",
    "This experiment tests the core capability of diffusion models: **learning multimodal probability distributions**.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **If PASS (Bimodal)**: The diffusion model correctly learns that:\n",
    "   - The same market condition can lead to different outcomes\n",
    "   - The model captures uncertainty through the distribution shape\n",
    "   - This is fundamentally different from MSE regression\n",
    "\n",
    "2. **If FAIL (Collapsed)**: The model has degenerated to:\n",
    "   - Predicting the mean of all possible outcomes\n",
    "   - Lost the ability to represent multiple modes\n",
    "   - Behaves like a standard regression model\n",
    "\n",
    "### Implications for Stock Prediction\n",
    "\n",
    "Real stock markets often have multimodal return distributions:\n",
    "- Same technical pattern can lead to breakout OR breakdown\n",
    "- Uncertainty around key events (earnings, FOMC)\n",
    "- Regime changes in market conditions\n",
    "\n",
    "A diffusion model that passes this test has the potential to:\n",
    "- Provide realistic uncertainty estimates\n",
    "- Identify scenarios with high outcome ambiguity\n",
    "- Support better risk management decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for later analysis\n",
    "import json\n",
    "\n",
    "experiment_results = {\n",
    "    'config': {\n",
    "        'n_samples': N_SAMPLES,\n",
    "        'seq_length': SEQ_LENGTH,\n",
    "        'n_features': N_FEATURES,\n",
    "        'target_value': TARGET_VALUE,\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'inference_samples': N_INFERENCE_SAMPLES\n",
    "    },\n",
    "    'results': {\n",
    "        'pct_bullish': float(results['pct_bullish']),\n",
    "        'pct_bearish': float(results['pct_bearish']),\n",
    "        'pct_neutral': float(results['pct_neutral']),\n",
    "        'bimodal_score': float(results['bimodal_score']),\n",
    "        'spread_score': float(results['spread_score']),\n",
    "        'mean': float(results['mean']),\n",
    "        'std': float(results['std'])\n",
    "    },\n",
    "    'final_loss': float(losses[-1]),\n",
    "    'is_bimodal': bool(is_bimodal),\n",
    "    'is_collapsed': bool(is_collapsed)\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_path = project_root / 'outputs' / 'bimodal_experiment_results.json'\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(experiment_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "print(\"\\nExperiment complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
